# Reflexive Python SDK - Examples

This directory contains example applications demonstrating various use cases of the Reflexive Python SDK.

## Running Examples

### Method 1: With Full CLI Integration (Recommended)

```bash
# Install Reflexive CLI (Node.js)
npm install -g reflexive

# Run any example with full AI capabilities
reflexive --debug simple_app.py
reflexive --debug web_server.py
reflexive --debug data_pipeline.py
```

This gives you:
- ‚úÖ Full `.chat()` functionality
- ‚úÖ Debugger with breakpoints
- ‚úÖ Web dashboard
- ‚úÖ Real-time log streaming
- ‚úÖ State inspection

### Method 2: Standalone

```bash
# Install Python SDK
pip install reflexive

# Run standalone (limited chat capabilities)
python simple_app.py
python web_server.py
python data_pipeline.py
```

## Examples

### 1. `simple_app.py` - Basic Usage

**What it demonstrates:**
- Creating a Reflexive instance
- Setting and tracking state
- Logging events
- Using `.chat()` to ask AI questions

**Run it:**
```bash
reflexive --debug simple_app.py
```

**What to try:**
- Watch the state updates in real-time
- See periodic AI analysis of application state
- Inspect logs through the dashboard

---

### 2. `web_server.py` - AI-Powered Web Server

**What it demonstrates:**
- Using `.chat()` to generate dynamic content
- Building an AI-native web API
- Tracking request metrics
- Inline AI generation

**Run it:**
```bash
reflexive --debug web_server.py
```

**Try these URLs:**
- http://localhost:8080/
- http://localhost:8080/story?topic=space+adventure
- http://localhost:8080/story?topic=mystery+detective

**What's cool:**
- Each story is generated by AI on-the-fly
- The AI has context about your app's state
- You can ask the AI questions via the CLI

**Example interaction:**
```
User (via CLI): "How many stories have been generated?"
AI: "Based on the state, 5 stories have been generated so far."
```

---

### 3. `data_pipeline.py` - Monitoring Example

**What it demonstrates:**
- Real-time monitoring of a data pipeline
- Using AI to analyze performance metrics
- Periodic health checks via `.chat()`
- Web UI for visualization

**Run it:**
```bash
reflexive --debug data_pipeline.py
```

**Features:**
- Simulates processing 100 data records
- Tracks success rate, errors, processing time
- AI analyzes performance every 20 records
- Full metrics available in web dashboard (http://localhost:3099)

**What to watch for:**
- How AI interprets error rates
- Performance recommendations from AI
- Real-time state updates

---

## Code Patterns

### Pattern 1: Inline AI Generation

```python
import reflexive

r = reflexive.make_reflexive()

# Generate content dynamically
def handle_request(topic):
    content = r.chat(f'Write about: {topic}')
    return content
```

**Use cases:**
- Content generation APIs
- Dynamic responses
- AI-assisted processing

### Pattern 2: State Monitoring

```python
import reflexive

r = reflexive.make_reflexive({'web_ui': True})

# Track application metrics
r.set_state('requests.count', count)
r.set_state('errors.rate', error_rate)

# Ask AI for insights
analysis = r.chat('Should I be concerned about the error rate?')
```

**Use cases:**
- Application monitoring
- Performance analysis
- Anomaly detection

### Pattern 3: Periodic Health Checks

```python
import reflexive
import time

r = reflexive.make_reflexive()

while processing:
    process_batch()

    if iteration % 10 == 0:
        health = r.chat('Analyze system health')
        if 'warning' in health.lower():
            alert_team(health)
```

**Use cases:**
- Background job monitoring
- ETL pipeline health
- System diagnostics

## Tips

### 1. State Management

Keep state small and informative:
```python
# Good: Metrics and counters
r.set_state('users.active', 42)
r.set_state('cache.hit_rate', 0.95)

# Bad: Large objects
r.set_state('all_users', huge_list)  # Don't do this!
```

### 2. Logging Strategy

Use structured logging:
```python
r.log('info', f'Processing batch {id}: {count} records')
r.log('warn', f'Slow query detected: {query_time}ms')
r.log('error', f'Failed to connect: {error}')
```

### 3. Chat Questions

Be specific:
```python
# Vague
r.chat('How are things?')

# Specific
r.chat('What is the error rate over the last 10 minutes?')
r.chat('Is memory usage trending up?')
```

### 4. Performance

Cache AI responses when appropriate:
```python
# Cache static content
@lru_cache(maxsize=100)
def get_ai_description(category):
    return r.chat(f'Describe: {category}')
```

## Next Steps

1. **Try the examples** with `reflexive --debug`
2. **Modify them** to fit your use case
3. **Build your own** AI-native app
4. **Read the docs** at `../README.md`

## Questions?

- üìö Read the full API docs: `../README.md`
- üèóÔ∏è Understand the design: `../DESIGN.md`
- üÜö Compare with Node.js: `../COMPARISON.md`
- üí¨ Ask the AI: `r.chat('How do I...')`
